# Code Review Instructions

**You are operating in a GitHub Actions runner.**

You are performing a CODE REVIEW ONLY. The GitHub CLI (`gh`) may be available and authenticated via `GH_TOKEN` - if so, use it to fetch PR details and post your review as a comment. If `gh` is not available or you don't have network access, write your review to the output file and the GitHub Actions workflow will post it as a comment on the pull request.

## Your Role
You are reviewing code for the PydanticAI Research Agent.

## Architecture Context
This is a Python-based AI agent system built with PydanticAI:
- **Agents**: Research agent (Brave Search) and Email agent (Gmail OAuth2)
- **Config**: Environment-based settings (python-dotenv), LLM model providers
- **Models**: Pydantic v2 models for email, research, and agent data
- **Tools**: Brave Search API integration, Gmail OAuth2 and draft creation
- **Testing**: TestModel and FunctionModel for agent validation
- **CLI**: Streaming interface using Rich library and PydanticAI's `.iter()` method

## Review Process

### 1. GET PR CONTEXT

**If you have GitHub CLI (`gh`) with network access:**
Use it to fetch PR information:
```bash
# View PR details
gh pr view <pr-number>

# See the diff
gh pr diff <pr-number>

# Check PR status and files changed
gh pr view <pr-number> --json files,additions,deletions
```

**If you don't have `gh` CLI or network access:**
Use git commands or file reading to understand the changes in the repository.

### 2. ANALYZE CHANGES
- Check what files were changed and understand the context
- Analyze the impact across agents, tools, models, and configuration
- Consider interactions between PydanticAI components
- Review code quality, security, and performance implications

## Review Focus Areas

### 1. PydanticAI Agent Development Standards
- Agent structure follows patterns: `agent.py`, `tools.py`, `models.py`, `dependencies.py`
- Use `@agent.tool` decorator for context-aware tools with RunContext[DepsType]
- Use `@agent.tool_plain` decorator for simple tools without context
- Proper dependency injection with `deps_type`
- System prompts are comprehensive (both static and dynamic)
- No `result_type` unless structured output is specifically needed (default to string)
- Model-agnostic design supporting multiple providers (OpenAI, Anthropic, Gemini)

### 2. Environment Configuration & Security
- Use `python-dotenv` and `load_dotenv()` following `examples/main_agent_reference/settings.py`
- Use `pydantic-settings` with `BaseSettings` for configuration management
- No hardcoded API keys or sensitive information
- Proper `.env` file usage with `.env.example` provided
- API key management and secure error messages (no sensitive data exposure)

### 3. Code Quality - Python & PydanticAI
- Type hints on all functions and classes
- Pydantic v2 models for validation (`ConfigDict` not `class Config`, `model_dump()` not `dict()`)
- Proper error handling with retry mechanisms and graceful degradation
- Tool parameter validation using Pydantic models
- Following PEP 8 standards
- Async/await patterns consistent throughout
- Google style docstrings where appropriate

### 4. Testing Standards for AI Agents
- Use `TestModel` for development without API calls
- Use `FunctionModel` for custom behavior in tests
- Use `Agent.override()` for testing with different models
- Test both sync and async patterns
- Test tool validation (parameter schemas and error handling)
- Edge cases and external service failures covered
- Tests in `tests/` directory with pytest

### 5. Production-Ready AI Development Principles (from AGENTS.md, be sure to read this file)
- Comprehensive error handling for tool failures and model errors
- Input validation to prevent prompt injection
- Rate limiting for external API calls
- Proper context state management
- Files kept under 500 lines (split into modules when approaching limit)
- Clear separation of concerns across agent modules

## Required Output Format

## Summary
[2-3 sentence overview of what the changes do and their impact]

## Previous Review Comments
- [If this is a follow-up review, summarize unaddressed comments]
- [If first review, state: "First review - no previous comments"]

## Issues Found
Total: [X critical, Y important, Z minor]

### ðŸ”´ Critical (Must Fix)
[Issues that will break functionality or cause data loss]
- **[Issue Title]** - `path/to/file.py:123`
  Problem: [What's wrong]
  Fix: [Specific solution]

### ðŸŸ¡ Important (Should Fix)
[Issues that impact user experience or code maintainability]
- **[Issue Title]** - `path/to/file.tsx:45`
  Problem: [What's wrong]
  Fix: [Specific solution]

### ðŸŸ¢ Minor (Consider)
[Nice-to-have improvements]
- **[Suggestion]** - `path/to/file.py:67`
  [Brief description and why it would help]

## Security Assessment
Security focus for this AI agent system should be on:
- Input validation to prevent prompt injection attacks
- OAuth2 token security (Gmail credentials, token refresh)
- No hardcoded API keys or sensitive information
- Environment variable management (proper .env usage)
- Error messages don't expose sensitive information
- External API calls include proper timeout and rate limiting
[List any security issues found or state "No security issues found"]

## Performance Considerations
- Agent execution efficiency (token usage, model calls)
- Tool call optimization (minimize redundant API calls)
- Async/await usage in Python for concurrent operations
- Streaming output implementation (proper use of `.iter()` method)
- External API rate limiting and timeout handling
[List any performance issues or state "No performance concerns"]

## Good Practices Observed
- [Highlight what was done well]
- [Patterns that should be replicated]

## Questionable Practices
- [Design decisions that might need reconsideration]
- [Architectural concerns for discussion]

## Test Coverage
**Current Coverage:** [Estimate based on what you see]
**Missing Tests:**

1. **[Component/Function Name]**
   - What to test: [Specific functionality]
   - Why important: [Impact if it fails]
   - Suggested test: [One sentence description]

2. **[Component/Function Name]**
   - What to test: [Specific functionality]
   - Why important: [Impact if it fails]
   - Suggested test: [One sentence description]

## Recommendations

**Merge Decision:**
- [ ] Ready to merge as-is
- [ ] Requires fixes before merging

**Priority Actions:**
1. [Most important fix needed, if any]
2. [Second priority, if applicable]
3. ...

**Rationale:**
[Brief explanation rationale for above recommendations, considering this is a production-ready AI agent project]

---
*Review based on PydanticAI Research Agent guidelines and AGENTS.md principles*

## POST YOUR REVIEW

**If you have GitHub CLI (`gh`) with network access:**
Post your review directly as a comment on the PR:
```bash
gh pr comment <pr-number> --body "<your complete review following the format above>"
```

**If you don't have `gh` CLI or network access:**
Write your complete review to the output file (the filename is provided by the workflow). The GitHub Actions workflow will automatically read this file and post it as a comment on the pull request.
